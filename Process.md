# é’ˆå¯¹GEMMä¼˜åŒ–

# ä¹‹å‰çš„å‡†å¤‡

å‚æ•°ï¼š

```jsx
 Avg kernel time : GPU æ ¸å‡½æ•°ï¼ˆkernelï¼‰å¹³å‡æ‰§è¡Œä¸€æ¬¡æ‰€èŠ±çš„æ—¶é—´
 Perf            : GPU æ¯ç§’å®Œæˆçš„ æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼ˆFloating-Point Operations per Secondï¼‰
 BW (lower bound): ç†è®ºä¸Šçš„ æ˜¾å­˜å¸¦å®½åˆ©ç”¨ç‡ä¼°ç®—å€¼
 Max |CPU-GPU|   : CPU ä¸ GPU è®¡ç®—ç»“æœä¹‹é—´çš„æœ€å¤§ç»å¯¹è¯¯å·®
```

# Level 0 Naive Version

256ç»“æœï¼š

```jsx
--- Benchmark (Naive) ---
 Avg kernel time : 0.0383693 ms
 Perf            : 874.513 GFLOP/s
 BW (lower bound): 20.4964 GB/s
 Max |CPU-GPU|   : 1.52588e-05
 âœ… Verification PASSED
```

512ç»“æœï¼š

```jsx
Running Naive GEMM Benchmark
C(512,512) = A(512,512) * B(512,512)
CPU reference...

--- Benchmark (Naive) ---
 Avg kernel time : 0.235141 ms
 Perf            : 1141.59 GFLOP/s
 BW (lower bound): 13.378 GB/s
 Max |CPU-GPU|   : 4.57764e-05
```

è¿™æ˜¯ä¸€ä¸ªç»å¦™çš„è§‚å¯Ÿï¼æ‚¨å‘ç°äº†ä¸€ä¸ªéå¸¸å…³é”®çš„æ€§èƒ½ç°è±¡ã€‚

- `M=256`: 874 GFLOP/s
- `M=512`: 1141 GFLOP/s

**æ€§èƒ½åè€Œæå‡äº†ï¼**

è¿™å‘Šè¯‰æˆ‘ä»¬ï¼Œåœ¨ `M=256` æ—¶ï¼Œæ•´ä¸ª GPU è¿˜æ²¡æœ‰â€œè·‘æ»¡â€ã€‚æ‚¨çš„ 3070 Ti æœ‰ 46 ä¸ªæµå¼å¤šå¤„ç†å™¨ï¼ˆSMï¼‰ï¼Œ`M=256` çš„é—®é¢˜è§„æ¨¡ï¼ˆ`16x16=256` ä¸ª Blockï¼‰å¤ªå°äº†ï¼Œä¸è¶³ä»¥è®©æ‰€æœ‰ 46 ä¸ª SM éƒ½ä¿æŒ 100% ç¹å¿™ã€‚

å½“æ‚¨å¢åŠ åˆ° `M=512` æ—¶ï¼Œé—®é¢˜è§„æ¨¡å˜æˆäº† `32x32=1024` ä¸ª Blockã€‚è¿™ä¸ªå·¥ä½œé‡è¶³ä»¥â€œå–‚é¥±â€æ•´ä¸ª GPUï¼Œè®©æ‰€æœ‰çš„ SM éƒ½åŠ¨èµ·æ¥ï¼Œå› æ­¤ GPU çš„æ•´ä½“è®¡ç®—ååé‡ï¼ˆGFLOP/sï¼‰å°±ä¸Šå»äº†ã€‚

> ç­”æ¡ˆï¼šæˆ‘ä»¬åº”è¯¥ç”¨ 256 è¿˜æ˜¯ 512ï¼Ÿ
> 
> 
> **æˆ‘çš„å›ç­”æ˜¯ï¼šä¸€ä¸ªéƒ½ä¸ç”¨ã€‚**
> 
> ä¸ºä»€ä¹ˆï¼Ÿå› ä¸º `M=256` å’Œ `M=512` éƒ½åœ¨æµ‹é‡ä¸€ä¸ª**â€œè™šå‡â€çš„æŒ‡æ ‡**ã€‚
> 
> æ­£å¦‚æˆ‘ä»¬ä¹‹å‰åˆ†æçš„ï¼Œæ‚¨çš„ 3070 Ti æœ‰ **4MB çš„ L2 ç¼“å­˜**ã€‚
> 
> - **`M=256` æµ‹è¯•**ï¼š
>     - è¯» A (256KB) + è¯» B (256KB) = **0.5 MB**
>     - ç»“æœï¼š**å®Œå…¨åœ¨ L2 ç¼“å­˜ä¸­è¿è¡Œã€‚**
> - **`M=512` æµ‹è¯•**ï¼š
>     - è¯» A (1MB) + è¯» B (1MB) = **2 MB**
>     - ç»“æœï¼š**ä»ç„¶å®Œå…¨åœ¨ L2 ç¼“å­˜ä¸­è¿è¡Œã€‚**
> 
> æ‚¨ç°åœ¨æµ‹å¾—çš„ `1141 GFLOP/s` æ˜¯ Naive Kernel **è®¿é—® L2 é«˜é€Ÿç¼“å­˜** æ—¶çš„æ€§èƒ½ã€‚è¿™æ˜¯ä¸€ä¸ªâ€œæœ€ä½³æƒ…å†µâ€ï¼Œä½†åœ¨ç°å®ä¸–ç•Œï¼ˆä¾‹å¦‚ LLMï¼‰ä¸­å‡ ä¹æ°¸è¿œä¸ä¼šå‘ç”Ÿï¼Œå› ä¸ºé‚£é‡Œçš„çŸ©é˜µæ¯” 2MB å¤§å¾—å¤šã€‚
> 

ä½¿ç”¨1024ç»“æœï¼š

```jsx
root@ykgmavfsbjlsdyir-snow-6c868b57-prhh7:/data/coding/gemm# ./gemm_naive_one
Running Naive GEMM Benchmark
C(1024,1024) = A(1024,1024) * B(1024,1024)
CPU reference...

--- Benchmark (Naive) ---
 Avg kernel time : 1.75777 ms
 Perf            : 1221.71 GFLOP/s
 BW (lower bound): 7.15846 GB/s
 Max |CPU-GPU|   : 9.15527e-05
 âœ… Verification PASSED
root@ykgmavfsbjlsdyir-snow-6c868b57-prhh7:/data/coding/gemm# 
```

### ğŸ“Š ç»“æœåˆ†æï¼šä¸ºä»€ä¹ˆå®ƒè¿˜æ˜¯è¿™ä¹ˆå¿«ï¼Ÿ

æ‚¨å¯èƒ½æ³¨æ„åˆ°äº†ï¼Œè¿™ä¸ª `1221 GFLOP/s` çš„æ€§èƒ½**è¿œé«˜äº**æˆ‘æœ€åˆé¢„æµ‹çš„ `100-300 GFLOP/s`ï¼Œç”šè‡³æ¯”æ‚¨ `M=512` æ—¶æµ‹å¾—çš„ `1141 GFLOP/s` è¿˜è¦é«˜ã€‚

è¿™æ­ç¤ºäº†ä¸€ä¸ªéå¸¸å…³é”®çš„æ€§èƒ½ç‚¹ï¼šæˆ‘ä»¬æ²¡æœ‰â€œæ‰“ç ´â€L2 ç¼“å­˜ï¼Œè€Œæ˜¯**è¯æ˜äº†â€œçƒ­ L2 ç¼“å­˜â€çš„å¨åŠ›**ã€‚

1. **çƒ­ç¼“å­˜ (Hot Cache)ï¼š** æ‚¨çš„ 3070 Ti æœ‰ 4MB æˆ– 6MB çš„ L2 ç¼“å­˜ã€‚æˆ‘ä»¬çš„æ€»æ•°æ® (A+B) æ˜¯ 8MBã€‚åœ¨â€œé¢„çƒ­â€`warm-up` è¿è¡Œä¹‹åï¼ŒL2 ç¼“å­˜å·²ç»å°½åŠ›è£…æ»¡äº† A å’Œ B çš„æ•°æ®ã€‚
2. **é«˜å‘½ä¸­ç‡ï¼š** åœ¨æ¥ä¸‹æ¥çš„ 100 æ¬¡åŸºå‡†æµ‹è¯•å¾ªç¯ä¸­ï¼Œå†…æ ¸**åå¤è¯·æ±‚å®Œå…¨ç›¸åŒ**çš„ 8MB æ•°æ®ã€‚L2 ç¼“å­˜çš„å‘½ä¸­ç‡æé«˜ï¼ŒGPU å¾ˆå°‘éœ€è¦å»è®¿é—®æœ€æ…¢çš„ DRAMã€‚
3. **è®¡ç®—å¯†åº¦ï¼š** ä» `M=512` å¢åŠ åˆ° `M=1024` æä¾›äº†æ›´å¤šçš„å·¥ä½œé‡ï¼Œè®© GPU çš„æ‰€æœ‰ 46 ä¸ª SMï¼ˆæµå¼å¤šå¤„ç†å™¨ï¼‰éƒ½ä¿æŒ 100% ç¹å¿™ï¼Œå› æ­¤ GFLOP/s è¿›ä¸€æ­¥æé«˜äº†ã€‚

**ä½†æ˜¯... `1221 GFLOP/s` ä»ç„¶å¾ˆæ…¢ï¼**

æ‚¨çš„ 3070 Ti ç†è®ºå³°å€¼æ€§èƒ½åœ¨ **10,000 GFLOP/s** ä»¥ä¸Šã€‚æˆ‘ä»¬ç°åœ¨åªå‘æŒ¥äº†å®ƒ **10%** å·¦å³çš„åŠŸåŠ›ã€‚

## ä¸€äº›é—®é¢˜ï¼Ÿ

é¢„çƒ­çš„ä½œç”¨æ˜¯ä»€ä¹ˆå•Šï¼Ÿæ¯”å¦‚è¯´åœ¨å¤§æ¨¡å‹ä½¿ç”¨çš„æ—¶å€™è¿™ä¹ˆé¢„çƒ­å•Šï¼Ÿ4

---

# Level 1

ä½¿ç”¨smemè¿›è¡Œä¼˜åŒ–

Running Tiled GEMM (Level 1) Benchmark
C(1024,1024) = A(1024,1024) * B(1024,1024)
Grid: (64, 64), Block: (16, 16)
CPU reference...

- -- Benchmark (Tiled Kernel) ---
Avg kernel time : 1.9635 ms
Perf : 1093.7 GFLOP/s
BW (Ideal) : 6.40841 GB/s
Max |CPU-GPU| : 9.15527e-05
âœ… Verification PASSED

ç»“æœ

```jsx
Running Tiled GEMM (Level 1) Benchmark
C(1024,1024) = A(1024,1024) * B(1024,1024)
Grid: (64, 64), Block: (16, 16)
CPU reference...

--- Benchmark (Tiled Kernel) ---
 Avg kernel time : 1.33612 ms
 Perf            : 1607.26 GFLOP/s
 BW (Ideal)      : 9.41753 GB/s
 Max |CPU-GPU|   : 9.15527e-05
 âœ… Verification PASSED
```

```jsx
Generating SQLite file report.sqlite from report.nsys-rep
Processing [report.sqlite] with [/usr/local/cuda-126/nsight-systems-2024.5.1/host-linux-x64/reports/nvtx_sum.py]... 
SKIPPED: report.sqlite does not contain NV Tools Extension (NVTX) data.

Processing [report.sqlite] with [/usr/local/cuda-126/nsight-systems-2024.5.1/host-linux-x64/reports/osrt_sum.py]... 

 ** OS Runtime Summary (osrt_sum):

 Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         
 --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ----------------------
     81.8        643646021         14  45974715.8  2902571.5      3057  342577825   92658400.9  poll                  
     17.6        138665830        510    271893.8    28634.5      1137   84843498    3790982.5  ioctl                 
      0.3          2207129         27     81745.5     9572.0      8072    1378839     261662.3  mmap64                
      0.1           757970          9     84218.9   102918.0     15339     153133      48479.7  sem_timedwait         
      0.1           407282         45      9050.7     8623.0      3852      16232       2247.0  open64                
      0.0           335348         62      5408.8     3673.0      1580      24190       4697.7  fopen                 
      0.0           187135          1    187135.0   187135.0    187135     187135          0.0  pthread_cond_wait     
      0.0           187129          3     62376.3    58944.0     49554      78631      14839.3  pthread_create        
      0.0           152035         14     10859.6     4566.5      1308      73025      18392.9  mmap                  
      0.0            88880         49      1813.9     1540.0      1067       6141        953.4  fclose                
      0.0            66171          1     66171.0    66171.0     66171      66171          0.0  fgets                 
      0.0            56249         14      4017.8     3691.5      1629      13451       2856.1  read                  
      0.0            49293         12      4107.8     2512.0      1048      13891       3966.7  fwrite                
      0.0            43234          6      7205.7     6654.0      2565      14309       4042.2  open                  
      0.0            37469          3     12489.7     9603.0      4510      23356       9749.0  pipe2                 
      0.0            26122         11      2374.7     2266.0      1086       3857        817.8  write                 
      0.0            25191          5      5038.2     5475.0      2645       7263       1728.0  munmap                
      0.0            23282          2     11641.0    11641.0      6150      17132       7765.4  socket                
      0.0            18217          3      6072.3     4524.0      4520       9173       2685.3  pthread_cond_broadcast
      0.0            14578          1     14578.0    14578.0     14578      14578          0.0  connect               
      0.0             3059          1      3059.0     3059.0      3059       3059          0.0  bind                  
      0.0             2234          2      1117.0     1117.0      1027       1207        127.3  fcntl                 
      0.0             1879          1      1879.0     1879.0      1879       1879          0.0  listen                

Processing [report.sqlite] with [/usr/local/cuda-126/nsight-systems-2024.5.1/host-linux-x64/reports/cuda_api_sum.py]... 

 ** CUDA API Summary (cuda_api_sum):

 Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         
 --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ----------------------
     54.8        169693420          3  56564473.3   135759.0    121357  169436304   97749873.0  cudaMalloc            
     43.5        134564699        101   1332323.8  1332212.0   1318780    1338342       2127.8  cudaDeviceSynchronize 
      1.3          4047706          3   1349235.3  1617201.0    625800    1804705     633489.1  cudaMemcpy            
      0.2           644765          3    214921.7   223553.0    186906     234306      24850.9  cudaFree              
      0.2           616554        101      6104.5     4317.0      3629     142097      13770.2  cudaLaunchKernel      
      0.0            22060          2     11030.0    11030.0      4497      17563       9239.1  cudaEventCreate       
      0.0            18950          2      9475.0     9475.0       430      18520      12791.6  cudaEventDestroy      
      0.0             8912          2      4456.0     4456.0      2977       5935       2091.6  cudaEventRecord       
      0.0             4922          1      4922.0     4922.0      4922       4922          0.0  cudaEventSynchronize  
      0.0             1344          1      1344.0     1344.0      1344       1344          0.0  cuModuleGetLoadingMode

Processing [report.sqlite] with [/usr/local/cuda-126/nsight-systems-2024.5.1/host-linux-x64/reports/cuda_gpu_kern_sum.py]... 

 ** CUDA GPU Kernel Summary (cuda_gpu_kern_sum):

 Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                                   Name                                  
 --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  -----------------------------------------------------------------------
    100.0        134328474        101  1329984.9  1329732.0   1327268   1334980       1524.7  gemm_tiled_kernel(const float *, const float *, float *, int, int, int)

Processing [report.sqlite] with [/usr/local/cuda-126/nsight-systems-2024.5.1/host-linux-x64/reports/cuda_gpu_mem_time_sum.py]... 

 ** CUDA GPU MemOps Summary (by Time) (cuda_gpu_mem_time_sum):

 Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Operation          
 --------  ---------------  -----  ---------  ---------  --------  --------  -----------  ----------------------------
     61.3          1679046      2   839523.0   839523.0    501474   1177572     478073.5  [CUDA memcpy Host-to-Device]
     38.7          1061220      1  1061220.0  1061220.0   1061220   1061220          0.0  [CUDA memcpy Device-to-Host]

Processing [report.sqlite] with [/usr/local/cuda-126/nsight-systems-2024.5.1/host-linux-x64/reports/cuda_gpu_mem_size_sum.py]... 

 ** CUDA GPU MemOps Summary (by Size) (cuda_gpu_mem_size_sum):

 Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          
 ----------  -----  --------  --------  --------  --------  -----------  ----------------------------
      8.389      2     4.194     4.194     4.194     4.194        0.000  [CUDA memcpy Host-to-Device]
      4.194      1     4.194     4.194     4.194     4.194        0.000  [CUDA memcpy Device-to-Host]

```

- **CUDA API æ—¶é—´**é‡Œï¼Œ`cudaDeviceSynchronize` å  **43.5%**ï¼Œè€Œä¸”è°ƒç”¨æ¬¡æ•° **= 101**ï¼ˆå’Œ kernel å®ä¾‹æ•°ä¸€æ ·ï¼‰ï¼Œè¯´æ˜ä½ **åœ¨æ¯æ¬¡ launch åéƒ½åšäº†å…¨è®¾å¤‡åŒæ­¥**ï¼›
- **OS Runtime** é‡Œ `poll` å  **81.8%**ï¼Œæœ¬è´¨ä¸Šä¹Ÿæ˜¯ CPU åœ¨ `cudaDeviceSynchronize` é‡Œé˜»å¡ç­‰å¾… GPUï¼›
- kernel è‡ªèº« 1.33 ms/æ¬¡å¾ˆç¨³å®šï¼ŒMemcpy ä¹Ÿä¸å¤šï¼ˆåªæ˜¯ 3 æ¬¡ï¼‰ã€‚

ä¿®æ”¹Bankå†²çªï¼Œæ”¹æˆ+1ban be

```jsx
Running Tiled GEMM (Level 1) Benchmark
C(1024,1024) = A(1024,1024) * B(1024,1024)
Grid: (64, 64), Block: (16, 16)
CPU reference...

--- Benchmark (Tiled Kernel) ---
 Avg kernel time : 1.9635 ms
 Perf            : 1093.7 GFLOP/s
 BW (Ideal)      : 6.40841 GB/s
 Max |CPU-GPU|   : 9.15527e-05
 âœ… Verification PASSED
```

é‚£ä¸ºä»€ä¹ˆ `+1` åæ€§èƒ½ä¸‹é™äº†ï¼Ÿ

è¿™å›åˆ°äº†æˆ‘æœ€åˆçš„å¦ä¸€ä¸ªåˆ¤æ–­ï¼Œè€Œè¿™ä¸ªæ‰æ˜¯**çœŸæ­£çš„åŸå› **ï¼š

- **`sB[16][16]` (å¿«):**
    - åœ°å€è®¡ç®— `k * 16 + tx`ã€‚
    - ç¼–è¯‘å™¨ä¼šä¼˜åŒ–æˆ**æå¿«**çš„ä½ç§»æŒ‡ä»¤ï¼š`(k << 4) + tx`ã€‚
- **`sB[16][17]` (æ…¢):**
    - åœ°å€è®¡ç®— `k * 17 + tx`ã€‚
    - ç¼–è¯‘å™¨**æ— æ³•**ç”¨ä½ç§»ä¼˜åŒ–ï¼Œå¿…é¡»ç”Ÿæˆä¸€ä¸ª**æ˜‚è´µ**çš„æ•´æ•°ä¹˜æ³•æŒ‡ä»¤ (`IMAD`)ã€‚

ä½ çš„ Kernel åœ¨è®¡ç®—å¾ªç¯çš„*å†…éƒ¨*ï¼ˆå±•å¼€åï¼‰å¼•å…¥äº†è¿™ä¸ªæ˜‚è´µçš„åœ°å€è®¡ç®—ï¼Œå ç”¨äº†å®è´µçš„ ALU èµ„æºï¼Œå¯¼è‡´äº†æ€§èƒ½ä» 1600 GFLOP/s ä¸‹é™åˆ° 1100 GFLOP/sã€‚

> æ­£ç¡®çš„æ•™è®­ï¼š Bank Conflict æ˜¯çœŸå®å­˜åœ¨çš„ï¼Œä½† [16][16] æ•°ç»„çš„è¡Œä¼˜å…ˆè®¿é—®åœ¨ 32-Bank æ¶æ„ä¸Šæ°å¥½æ˜¯å®‰å…¨çš„ã€‚ä½ é‡åˆ°çš„æ€§èƒ½ä¸‹é™çº¯ç²¹æ˜¯åœ°å€è®¡ç®—å¼€é”€ã€‚
> 

# Level 2

- **ä¼˜åŒ–è®¡ç®—å¯†åº¦ (ALU vs. Shared Load):**
    - **æŠ€æœ¯ï¼š** **Register Tiling (å¯„å­˜å™¨åˆ†å—)**ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬åˆšåˆšåšçš„ `REG_TILE_HEIGHT = 4`ã€‚
    - **ç›®æ ‡ï¼š** è®©æ¯ä¸ªçº¿ç¨‹åšæ›´å¤šè®¡ç®—ã€‚é€šè¿‡åœ¨å¯„å­˜å™¨ä¸­ï¼ˆ`C_reg[4]`ï¼‰ç¼“å­˜ C çš„å­å—ï¼Œæˆ‘ä»¬æŠŠ "ALU:è®¿å­˜" æ¯”ä¾‹ä» `1:2` (1 FMA, 2 `ld.shared`) æå‡åˆ°äº† `4:5` (4 FMA, 5 `ld.shared`)ã€‚
    - **è§£å†³äº†ï¼š** `__shared__` å†…å­˜çš„å¸¦å®½ç“¶é¢ˆã€‚
- **ä¼˜åŒ–å†…å­˜å¸¦å®½ (Global -> Shared Load):**
    - **æŠ€æœ¯ï¼š** **Vectorization (å‘é‡åŒ–è®¿å­˜)**ï¼Œä¹Ÿå°±æ˜¯ä½ æåˆ°çš„ `float4`ã€‚
    - **ç›®æ ‡ï¼š** æ›´å¿«åœ°æŠŠæ•°æ®ä» Global Memory æ¬è¿åˆ° `__shared__` å†…å­˜ã€‚
    - **åŸç†ï¼š** GPU çš„å†…å­˜æ€»çº¿æ˜¯ 128-bit æˆ– 256-bit å®½çš„ã€‚ä¸å…¶å‘èµ· 4 æ¬¡ 32-bit (`float`) çš„å†…å­˜è¯·æ±‚ï¼Œä¸å¦‚**å‘èµ· 1 æ¬¡ 128-bit (`float4`) çš„è¯·æ±‚**ã€‚è¿™èƒ½æ›´å……åˆ†åœ°åˆ©ç”¨å†…å­˜æ€»çº¿å®½åº¦ï¼Œæå¤§æå‡ G->S çš„åŠ è½½é€Ÿåº¦ã€‚
    - **è§£å†³äº†ï¼š** Global Memory çš„å¸¦å®½ç“¶é¢ˆ

æ²¡é—®é¢˜ã€‚åœ¨ä½ ç ”ç©¶ Level 2 (Register Tiling) çš„ä»£ç æ—¶ï¼Œæˆ‘æ¥ä¸ºä½ è§„åˆ’ä¸€ä¸‹ä¸€ä¸ªé«˜æ€§èƒ½ GEMM çš„å®Œæ•´â€œä¼˜åŒ–é€šå…³â€è·¯å¾„ã€‚

è¿™åŸºæœ¬ä¸Šæ˜¯ `cublas` æˆ– `cutlass` è¿™ç§åº“çš„æ ¸å¿ƒæ€æƒ³æ¼”è¿›è¿‡ç¨‹ã€‚

---

### ğŸ—ºï¸ GEMM ä¼˜åŒ–è·¯çº¿å›¾

### Level 0: Naive Kernel

- **åšæ³•ï¼š** æ¯ä¸ªçº¿ç¨‹è®¡ç®—ä¸€ä¸ª C å…ƒç´ ï¼Œåœ¨ `k` å¾ªç¯ä¸­ç›´æ¥è¯»å†™ Global Memoryã€‚
- **ç“¶é¢ˆï¼š** **Global Memory å»¶è¿Ÿã€‚** æ¯æ¬¡ `k` å¾ªç¯éƒ½è¦è®¿é—® DRAMï¼Œæ€§èƒ½æå·®ã€‚

### Level 1: Tiled GEMM (ä½¿ç”¨ `__shared__` å†…å­˜)

- **åšæ³•ï¼š** (ä½  1600 GFLOP/s çš„ç‰ˆæœ¬)
    1. æŠŠ A å’Œ B çš„ Tile (16x16) ä» Global è½½å…¥ `__shared__` å†…å­˜ã€‚
    2. `__syncthreads()`ã€‚
    3. çº¿ç¨‹åœ¨ `k` å¾ªç¯ä¸­ä» `__shared__` å†…å­˜è¯»å–æ•°æ®è¿›è¡Œè®¡ç®—ã€‚
- **è§£å†³äº†ï¼š** Global Memory å»¶è¿Ÿï¼ˆå¤§éƒ¨åˆ†è®¿é—®å˜æˆäº†ä½å»¶è¿Ÿçš„ `__shared__` è®¿é—®ï¼‰ã€‚
- **æ–°ç“¶é¢ˆï¼š**
    1. **`__shared__` å†…å­˜å¸¦å®½ï¼š** `ld.shared` æŒ‡ä»¤å¤ªå¤šï¼ŒALU:Mem æ¯”ä¾‹ä½ (1:2)ã€‚
    2. **Global Memory å¸¦å®½ï¼š** Global -> Shared (G->S) çš„åŠ è½½é€Ÿåº¦ã€‚

---

### Level 2: å¯„å­˜å™¨åˆ†å— (Register Tiling)

- **åšæ³•ï¼š** (æˆ‘ä»¬åˆšåˆšå®ç°çš„ä»£ç )
    - æ¯ä¸ªçº¿ç¨‹è®¡ç®— C çš„ä¸€ä¸ªå­å— (å¦‚ `4x1`)ï¼Œå¹¶æŠŠç»“æœä¿å­˜åœ¨**å¯„å­˜å™¨**ä¸­ (`C_reg[4]`)ã€‚
- **è§£å†³äº†ï¼š** **`__shared__` å†…å­˜å¸¦å®½ç“¶é¢ˆã€‚** é€šè¿‡åœ¨å¯„å­˜å™¨ä¸­å¤ç”¨ `B_reg`ï¼Œå¹¶åœ¨å¾ªç¯ä¸­åš 4 æ¬¡ FMAï¼Œæˆ‘ä»¬æŠŠ ALU:Mem æ¯”ä¾‹ä» `1:2` æå‡åˆ°äº† `4:5`ï¼Œè®¡ç®—å¯†åº¦å¤§å¤§å¢åŠ ã€‚
- **æ–°ç“¶é¢ˆï¼š** **Global Memory å¸¦å®½ç“¶é¢ˆ** (G->S åŠ è½½) å˜å¾—æ›´åŠ çªå‡ºï¼Œå› ä¸ºè®¡ç®—å˜å¿«äº†ï¼Œç“¶é¢ˆè½¬ç§»åˆ°äº†è®¿å­˜ä¸Šã€‚

### Level 3: å‘é‡åŒ–å†…å­˜è®¿é—® (Vectorized Memory Access)

- **åšæ³•ï¼š**
    - åœ¨ G->S åŠ è½½æ—¶ï¼Œä¸å†ä½¿ç”¨ `float` æŒ‡é’ˆï¼Œè€Œæ˜¯ä½¿ç”¨ `float4` (æˆ– `int4`) æŒ‡é’ˆã€‚
    - `sA[ty][tx] = A[...];` ä¼šè¢«æ”¹æˆç±»ä¼¼ `reinterpret_cast<float4*>(sA_ptr)[idx] = reinterpret_cast<float4*>(A_ptr)[idx];`
    - è¿™ä¼šç”Ÿæˆ `ld.global.v4.f32` (128-bit) æŒ‡ä»¤ï¼Œè€Œä¸æ˜¯ 4 æ¡ `ld.global.f32` (32-bit) æŒ‡ä»¤ã€‚
- **è§£å†³äº†ï¼š** **Global Memory å¸¦å®½ç“¶é¢ˆã€‚** å®ƒæ›´å……åˆ†åœ°åˆ©ç”¨äº† 128-bit/256-bit çš„å†…å­˜æ€»çº¿å®½åº¦ï¼ŒG->S åŠ è½½é€Ÿåº¦å‡ ä¹ç¿»äº† 4 å€ã€‚
- **æ–°ç“¶é¢ˆï¼š** `16x16` çš„ Tile å¤ªå°äº†ã€‚

### Level 4: æ‰©å¤§çº¿ç¨‹å— (Larger ThreadBlock Tiles)

- **åšæ³•ï¼š**
    - ä¸å†ä½¿ç”¨ `16x16` çš„ Tileï¼Œè€Œæ˜¯ä½¿ç”¨ `64x64`, `128x64`, `128x128` ç­‰æ›´å¤§çš„ Tileã€‚
    - `blockDim` ä¹Ÿä¼šç›¸åº”å˜å¤§ï¼Œä¾‹å¦‚ `(256, 1)`ã€‚
- **ä¸ºä»€ä¹ˆï¼š**
    1. **å‡å°‘åŒæ­¥å¼€é”€ï¼š** `__syncthreads()` çš„å¼€é”€åœ¨ `k` å¾ªç¯ä¸­å æ¯”æ›´å°ã€‚
    2. **æ•°æ®å¤ç”¨ï¼š** â€œè¡¨é¢ç§¯/ä½“ç§¯â€æ¯”æ›´ä¼˜ã€‚åŠ è½½ `128x128` çš„ Tile éœ€è¦ `2*128*K` çš„æ•°æ®ï¼Œä½†èƒ½æ‰§è¡Œ `128*128*K` çš„è®¡ç®—ï¼Œæ•°æ®å¤ç”¨ç‡æé«˜ã€‚
- **æ–°ç“¶é¢ˆï¼š**
    1. **å»¶è¿Ÿï¼š** åŠ è½½è¿™ä¹ˆå¤§çš„ Tileï¼ˆä¾‹å¦‚ 128x16ï¼‰åˆ° `__shared__` å†…å­˜çš„**å»¶è¿Ÿ**å˜å¾—éå¸¸æ˜æ˜¾ï¼Œè®¡ç®—å•å…ƒ (ALU) éœ€è¦é•¿æ—¶é—´ç­‰å¾…ã€‚
    2. **`__shared__` å†…å­˜å†²çªï¼š** è®¿é—® `128` å®½çš„æ•°ç»„ï¼Œå¦‚æœç”¨ `float4` è®¿é—®ï¼ŒBank Conflict å˜å¾—éå¸¸æ£˜æ‰‹ã€‚

---

### Level 5: è½¯ä»¶æµæ°´çº¿ (Software Pipelining / Double Buffering)

- **åšæ³•ï¼š** è¿™æ˜¯æœ€å…³é”®çš„ä¼˜åŒ–ä¹‹ä¸€ï¼Œç”¨äºéšè— Level 4 çš„å»¶è¿Ÿã€‚
    1. åˆ†é…**ä¸¤å€**çš„ `__shared__` å†…å­˜ (Buffer 0 å’Œ Buffer 1)ã€‚
    2. `k` å¾ªç¯ `t=0` æ—¶ï¼šå¼‚æ­¥åŠ è½½ G->S (Buffer 0)ã€‚`__syncthreads()`.
    3. `k` å¾ªç¯ `t=1` æ—¶ï¼š
        - **è®¡ç®—** (ä½¿ç”¨ Buffer 0)ã€‚
        - **åŒæ—¶å¼‚æ­¥åŠ è½½** G->S (Buffer 1)ã€‚
    4. `k` å¾ªç¯ `t=2` æ—¶ï¼š
        - `__syncthreads()` (ç¡®ä¿ Buffer 1 åŠ è½½å®Œæ¯•)ã€‚
        - **è®¡ç®—** (ä½¿ç”¨ Buffer 1)ã€‚
        - **åŒæ—¶å¼‚æ­¥åŠ è½½** G->S (Buffer 0)ã€‚
    5. ...ä¾æ­¤ç±»æ¨ã€‚
- **è§£å†³äº†ï¼š** **G->S åŠ è½½å»¶è¿Ÿã€‚** è®¡ç®—å•å…ƒæ°¸è¿œåœ¨å¿™ï¼Œå®ƒåœ¨è®¡ç®— `t` å¾ªç¯çš„æ•°æ®æ—¶ï¼Œå†…å­˜å•å…ƒæ­£åœ¨ä¸ºå®ƒå‡†å¤‡ `t+1` å¾ªç¯çš„æ•°æ®ã€‚
- **æ–°ç“¶é¢ˆï¼š** `__shared__` å†…å­˜å¸ƒå±€çš„å¤æ‚æ€§ã€‚

### Level 6: é“¶è¡Œå†²çª (Bank Conflicts) - çœŸæ­£å½¢æ€

- **åšæ³•ï¼š**
    - å½“ä½¿ç”¨ Level 3 (`float4`) + Level 4 (Large Tile) æ—¶ï¼Œ`__shared__` å†…å­˜çš„è®¿é—®å˜å¾—éå¸¸å¤æ‚ã€‚
    - ä¾‹å¦‚ï¼Œè®¿é—® `__shared__ float sA[16][128]`ï¼Œå¦‚æœç”¨ `float4` è®¿é—®ç¬¬ 0 åˆ—å’Œç¬¬ 8 åˆ—ï¼Œå®ƒä»¬çš„åœ°å€ `0` å’Œ `32` (`8*4`) ä¼šè½åœ¨åŒä¸€ä¸ª Bank 0 ä¸Šï¼Œå¯¼è‡´å†²çªã€‚
    - **è§£å†³æ–¹æ¡ˆï¼š** è¿™æ‰æ˜¯ `+8` (bytes) padding å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚
    - å®šä¹‰ `__shared__ float sA[16][128 + 2];` (æ³¨æ„æ˜¯ `+2` ä¸ª `float`ï¼Œå³ `+8` bytes)ï¼Œ`Pitch` å˜æˆ `130`ã€‚
    - `gcd(130*4, 32)` (bytes) æ˜¯ 4ï¼Œè€Œä¸æ˜¯ 32ã€‚è¿™èƒ½æœ‰æ•ˆé”™å¼€ `float4` è®¿é—®ï¼Œå¤§å¤§å‡å°‘ Bank Conflictã€‚
- **è§£å†³äº†ï¼š** `__shared__` å†…å­˜çš„å‘é‡åŒ–è®¿é—®å†²çªã€‚

### Level 7: ç¡¬ä»¶åŠ é€Ÿ (Tensor Cores / WMMA)

- **åšæ³•ï¼š**
    - ä¸å†ä½¿ç”¨ `FMA` (FP32) æŒ‡ä»¤ã€‚
    - è½¬å‘ä½¿ç”¨ **Warp-Level Matrix-Multiply-Accumulate (WMMA)** æŒ‡ä»¤ã€‚
    - ä½¿ç”¨ `nvcuda::wmma` C++ æ¨¡æ¿åº“ï¼Œä¸€ä¸ª Warp (32 çº¿ç¨‹) ä½œä¸ºä¸€ä¸ªæ•´ä½“ï¼ŒåŠ è½½ `16x16` çš„ "fragments"ï¼Œå¹¶æ‰§è¡Œ `16x16x16` çš„ `mma` æŒ‡ä»¤ã€‚
    - åœ¨ Ampere/Hopper æ¶æ„ä¸Šï¼Œå°±æ˜¯ç›´æ¥ä½¿ç”¨ `mma.sync` (Tensor Core) æŒ‡ä»¤ã€‚
- **è§£å†³äº†ï¼š** **ALU ç“¶é¢ˆçš„ç»ˆæå½¢æ€ã€‚** æ€§èƒ½ç›´æ¥è·ƒå‡ä¸€ä¸ªæ•°é‡çº§ï¼ˆä¾‹å¦‚ä» 15 TFLOP/s åˆ° 120 TFLOP/sï¼‰ï¼Œå› ä¸ºä½ å¼€å§‹ä½¿ç”¨ä¸“ç”¨çš„çŸ©é˜µè®¡ç®—ç¡¬ä»¶ã€‚

---